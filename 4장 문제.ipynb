{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371ae2c9",
   "metadata": {},
   "source": [
    "# 4-1 선형 모델\n",
    "\n",
    "## Q1\n",
    "\n",
    "다음 중 선형 모델에 대한 설명으로 옳지 않은 것은 무엇인가요?\n",
    "\n",
    "1) 머신 러닝에서의 선형 회귀 모델은 예측과 분류 문제의 해결과 성능 향상에 중점을 둡니다.\n",
    "\n",
    "2) 선형 모델은 계수의 크기가 커지는 것을 억제하는 규제를 통해 모델의 분산과 편향을 제어합니다. \n",
    "\n",
    "3) 선형 회귀 모델을 링크 함수를 통해 출력 변수의 분포를 바꾸어 적용 영역을 확장할 수 있습니다.\n",
    "\n",
    "4) 단일 로지스틱 회귀 모델로 다중 클래스 분류에 적용하기 위해 로지스틱 함수를 사용합니다.\n",
    "\n",
    "<font color='red'> 정답 4 번: 단일 로지스틱 회귀 모델로 다중 클래스 분류에 적용할 때는 소프트맥스(Softmax)함수를 사용합니다.</font>\n",
    "\n",
    "\n",
    "## Q2\n",
    "\n",
    "선형 회귀 모델의 규제에 대한 설명으로 옳지 않은 것은 무엇인가요?\n",
    "\n",
    "1) 릿지(Ridge) 회귀에서는 L2 규제가 사용되며, 이는 모델의 가중치에 대한 제곱합을 패널티로 사용합니다. \n",
    "\n",
    "2) 라쏘(Lasso) 회귀에서는 L1 규제가 사용되며, 이는 모델의 가중치에 대한 절대값의 합을 패널티로 사용합니다. \n",
    "\n",
    "3) 규제가 적용된 선형 회귀 모델은 일반적으로 과적합을 방지하여 모델의 일반화 성능을 향상시키기 위해 사용합니다. \n",
    "\n",
    "4) 릿지(Ridge) 회귀 모델은 라쏘(Lasso) 회귀 모델 보다 회귀 계수가 0에 근접한 경우가 많으며, 이를 바탕으로 속성 선택의 방법으로 활용할 수 있습니다.\n",
    "\n",
    "<font color='red'> 정답 4 번: 회귀 계수가 0에 가깝게 되는 경우는 라쏘(Lasso) 회귀 모델이 더 빈번하게 발생합니다.</font>\n",
    "\n",
    "## Q3\n",
    "\n",
    "다음 손실 함수를 가지는 선형 회귀 모델에서 과적합이 발생했을 때의 조치로 가장 적합한 것은 무엇인가요?\n",
    "\n",
    "\n",
    "$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{i=1}^{n} w_i^2$\n",
    "\n",
    "n은 데이터 포인트의 개수,  $y_i$는 실제 대상 변수, $\\hat{y_i}$: 모델에 의한 예측값, $w_i$: 선형 회귀 모델의 회귀 계수\n",
    "\n",
    "\n",
    "1) α 값을 증가시킵니다. \n",
    "\n",
    "2) α 값을 감소시킵니다.\n",
    "\n",
    "3) α 값을 0으로 만듭니다. \n",
    "\n",
    "4) 취할 수 있는 조치가 없습니다.\n",
    "\n",
    "<font color='red'> 정답 1: 규제 계수인 α의 키워 회귀 계수의 절대 크기를 억제 하여 모델의 분산을 낮춥니다. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf7c1e",
   "metadata": {},
   "source": [
    "# 4-2  의사 결정 나무\n",
    "\n",
    "\n",
    "## Q1\n",
    "\n",
    "의사결정나무(Decision Tree)를 구성하는 방식중에 레벨 기준(Level-Wise)과 성능 우선(Best-First)에 대한 설명으로 적합하지 않은 것은 무엇입니까?\n",
    "\n",
    "1) 레벨기준(Level-Wise) 방식은 병렬 처리에 성능 우선(Best-First) 방식 보다 용이합니다.\n",
    "\n",
    "2) 성능 우선(Best-First)은 레벨 기준(Level-Wise) 방식보다 계산량이 많습니다.\n",
    "\n",
    "3) 레벨 기준(Level-Wise) 방식은 성능 우선(Best-First) 방식은 보다 과적합이 일어 나기 쉽습니다.\n",
    "\n",
    "4) 레벨 기준(Level-Wise) 방식은 성능 우선(Best-First) 방신에 비해 비효율적인 분할을 지닐 가능성이 높습니다.\n",
    "\n",
    "<font color='red'>정답 3 번: 성능 우선(Best-First) 방식이 학습 데이터에 더 가깝게 맞출습니다. 이로 인해 과적합이 보다 쉽게 발생할 수 있습니다.</font>\n",
    "\n",
    "\n",
    "## Q2\n",
    "\n",
    "의사결정나무의 모델이 과적합 상태라고 판단이 들 때, 이를 해결하기 위한 방안으로 적합하지 않은 것은 무엇인가요?\n",
    "\n",
    "1) 최대 깊이를 크기를 줄입니다.\n",
    "\n",
    "2) 분기(내부 노드)를 생성하기 위한 최소의 표본수를 줄입니다. \n",
    "\n",
    "3) 리프 노드의 최대 수를 줄입니다.\n",
    "\n",
    "4) 리프 노드 생성에 필요한 최소 표본수를 늘립니다.\n",
    "\n",
    "<font color='red'>정답 2 번: 분기를 생성하기 위한 최소 표본수를 줄인 다면, 더 많은 내부 노드가 생성될 수 있어 모델의 복잡도가 높아져서, 과적합이 더 심해질 수 있습니다.</font>\n",
    "\n",
    "## Q3 \n",
    "\n",
    "의사결정나무의 특징이라 보기에 어려운 것은 무엇입니까?\n",
    "\n",
    "1) 데이터의 전처리의 필요성이 상대적으로 많습니다.\n",
    "\n",
    "2) 모델에 가정을 두고 있지 않으므로, 다양한 문제에 적용할 수 있습니다. \n",
    "\n",
    "3) 데이터의 작은 변화에도 모델의 출력 결과가 크게 변하기 쉽습니다.\n",
    "\n",
    "4) 직관적인 해석을 할 수 있는 형태를 보입니다.\n",
    "    \n",
    "<font color='red'> 정답 1 번: 데이터의 전처리 필요성이 다른 모델에 비해 적습니다. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7afc01",
   "metadata": {},
   "source": [
    "# 4-3 나이브 베이즈, K 최근접 이웃\n",
    "\n",
    "## Q1 \n",
    "\n",
    "다음 중 나이브 베이즈 모델에 대한 설명 중 옳지 않은 것은 무엇인가요?\n",
    "\n",
    "1) 규모가 작은 데이터에 대한 좋은 성능을 보이지만, 학습과 분류에 연산량이 많아 규모가 커질 수록 적용이 어렵습니다.\n",
    "\n",
    "2) 단어 빈도수 기반의 텍스트 분류, 추천 등 동일한 척도의 여러 속성을 지닌 데이터에 강점이 있습니다. \n",
    "\n",
    "3) 데이터 간의 복잡한 상관 관계나 패턴을 캡처하지 못하여, 이러 인한 성능 저하를 가져 올 수 있습니다.\n",
    "\n",
    "4) 클래스에 대해 발생 빈도가 매우 적은 일부 속성에 인해 지나치게 낮은 예측 확률로 인한 성능 저하 현상을 가법 평활화(Additive smoothing, Laplace smoothing)으로 완화시킬 수 있습니다.\n",
    "\n",
    "<font color='red'>정답 1 번:나이브 베이즈는 연산량이 많이 필요한 모델은 아닙니다.</font>\n",
    "\n",
    "## Q2\n",
    "\n",
    "K-최근접 이웃 모델에 대한 설명 중 옳지 않은 것은 무엇인가요?\n",
    "\n",
    "1) 거리 측정 방법에 따라 결과가 달라지므로, 적절한 거리 메트릭을 선택하는 것이 중요합니다.\n",
    "\n",
    "2) 데이터가 많을 때 효과적으로 작동하며, 특히 고차원 데이터에의 효율성이 높습니다.\n",
    "\n",
    "3) 과적합 상태라면, 근접 이웃의 수를 늘려 모델의 분산을 줄입니다. \n",
    "\n",
    "4) 훈련 과정이 없으며, 모든 계산은 예측시에 이루어집니다. \n",
    "\n",
    "<font color='red'> 정답 2: 데이터가 많으면 예측시 연산 자원이 많이 필요하게 되며, 거리로 패턴을 나타내기 위해서는 차원이 높아질 수록 보다 많은 데이터가 필요합니다.(차원의 저주 현상에 취약합니다.)</font>\n",
    "\n",
    "## Q3\n",
    "\n",
    "나이브 베이즈 모델로 스팸 여부를 분류하는 분류 모델을 만들었습니다. 나이브 베이즈 모델에서는 아래 가법 평활화를 사용하고 있습니다. \n",
    "\n",
    "이를 바탕으로 아래 설명 중에서 적합한 내용을 모두 고른 것은 무엇인가요?\n",
    "\n",
    "$\\hat{\\theta}_{ci} = \\frac{ N_{ci} + \\alpha}{N_c + \\alpha n}$\n",
    "\n",
    "\n",
    "(ㄱ) α를 높이면 평가 성능이 높아지다, 어느 수준 이상이면 α 낮아지기 시작합니다.\n",
    "\n",
    "(ㄴ) 가법 평활화를 통해 학습 과정에서 연산량을 줄일 수 있습니다.\n",
    "\n",
    "(ㄷ) 가법 평활화는 소수 클래스에 대한 분류 성능을 높이기 위해 사용합니다.\n",
    "\n",
    "(ㄹ) 가법 평활화는 클래스에 대한 속성의 조건부 확률이 0이 되는 것을 방지합니다.\n",
    "\n",
    "1) (ㄱ), (ㄴ)\n",
    "\n",
    "2) (ㄴ), (ㄷ)\n",
    "\n",
    "3) (ㄷ), (ㄹ)\n",
    "\n",
    "4) (ㄱ), (ㄹ)\n",
    "\n",
    "<font color='red'> 정답 4: 가법 평활 계수를 높일 수록 편향은 커집니다. 클래스 별 속성의 출현 빈도수에 적용되어 빈도가 일정 수 이상 나오게 되어 속성에 대한 조건부 확률이 0이 되는 것을 방지하고, 이에 따른 클래스 확률까지 0이 되는 것을 방지합니다. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70906b",
   "metadata": {},
   "source": [
    "# 4-4 SVM\n",
    "\n",
    "## Q1 \n",
    "\n",
    "SVM에 대한 설명으로 잘못된 것은 무엇인가요?\n",
    "\n",
    "1) SVM 모델에 지지 벡터(Support Vector)가 많을 수록 일반화가 성능이 높습니다.\n",
    "\n",
    "2) Linear SVM 모델의 학습 산출물은 선형 판별함수로 대체할 수 있습니다.\n",
    "\n",
    "3) 커널 트릭은  고차원 속성들을 직접 생성하지 않고도 비선형 패턴을 분류할 수 있도록 해줍니다.\n",
    "\n",
    "4) 소프트 마진 판별기는 최대 마진 분류기의 분류 마진에 대한 조건을 완화시켜 선형 분리가 가능하지 않은 경우에도 적용가능하게 합니다.\n",
    "\n",
    "<font color='red'> 정답 1:  지지 벡터(Support Vector)가 많을 수록 모델의 복잡도는 높아지고 일반화 성능은 떨어집니다. </font>\n",
    "\n",
    "## Q2\n",
    "\n",
    "다음은 Soft Margin을 갖는 최대 마진 분류기의 목적함수 입니다. 이에 대한 설명으로 적절한 것은 무엇인 가요? \n",
    "\n",
    "$\\frac{1}{2}||w||_2^2 + C\\sum_{i=1}^n\\xi_i$, w는 분류 평면과 수직인 벡터, $\\xi_i$: i 데이터 포인트의 슬랙 변수 이고, n은 데이터 포인트수 입니다. \n",
    "\n",
    "1) 평가 데이터의 성능은 C를 높힐 수록 증가하다가, C가 일정 수준이 넘어서면 감소하는 경향을 보입니다. \n",
    "\n",
    "2) 슬랙 변수가 클수록 데이터 포인트의 분류 마진은 큽니다.\n",
    "\n",
    "3) Soft Margin 분류기는 Lagrange Dual을 적용할 수 없습니다.\n",
    "\n",
    "4) C는 Lagrage multiplier의 학습율이고, C를 높일 수록 업데이트 반영률이 높아져 빨리 수렴합니다.\n",
    "\n",
    "<font color='red'> 정답 1:  C를 높이면 학습 데이터를 분리력이 높아져서 보다 학습 데이터에 적합하게 됩니다. 따라서 높일 수록 최적합 상태가 되다가 과적합 상태가 됩니다</font>\n",
    "\n",
    "## Q3\n",
    "\n",
    "아래 손실함수를 최적화하는 방안을, \n",
    "\n",
    "$\\frac{1}{2}||w||_2^2 + C\\sum_{i=1}^n\\xi_i$, w는 분류 평면과 수직인 벡터, $\\xi_i$: i 데이터 포인트의 슬랙 변수 이고, n은 데이터 포인트수 입니다. \n",
    "\n",
    "Lagrange Dual을 통해 유도된 SVM으 손실 함수와 KKT 요건이 아래와 같을 때, SVM 학습후에 데이터 포인트에 대한 설명으로 맞는 것은 무엇인가요?\n",
    "\n",
    "$L(\\alpha) = max_{\\alpha} \\left(\\sum_{i=1}^n\\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j\\phi(x_i^Tx_j)\\right)$\n",
    "\n",
    "where, \n",
    "\n",
    "> $y_i(w^Tx_i + b) - 1 + \\xi_i \\ge 0, i=1, ... n$\n",
    ">\n",
    "> $w = \\sum_{i=1}^n\\alpha_iy_ix_i$\n",
    ">\n",
    "> $\\alpha_i \\ge 0, i=1, ... n$\n",
    ">\n",
    "> $\\alpha_i\\left(y_i(w^Tx_i + b) - 1 + \\xi_i \\right) = 0, i=1, ... n$\n",
    ">\n",
    "> $\\gamma_𝑖\\xi_𝑖 = 0, i=1, ... n$\n",
    "\n",
    "\n",
    "1) $0 < \\alpha_i < C$ 이면 최대 마진 위의 지지 벡터(Support Vector) 입니다.\n",
    "\n",
    "2) $\\alpha_i = 0$ 이면 최대 마진 안 쪽의 지지 벡터(Support Vector)입니다. \n",
    "\n",
    "3) $\\alpha_i = C$ 이면 최대 마진 바깥 쪽의 데이터 포인트 입니다. \n",
    "\n",
    "4) $\\xi_i > 0$ 이면 모두 오분류 포인트 입니다. \n",
    "\n",
    "\n",
    "<font color='red'> 정답 1: 2) 가 최대 마진 바깥 쪽의 데이터 포인트이고,  3) 최대 마진 안 쪽의 지지 벡터(Support Vector) 입니다. 4) 모두 오분류 포인트는 아닙니다. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a9440",
   "metadata": {},
   "source": [
    "# 4-5 앙상블\n",
    "\n",
    "## Q1\n",
    "\n",
    "앙상블을 사용하는 목적은 무엇인가요?\n",
    "\n",
    "1) 모델의 복잡도를 줄여 예측 작업의 효율을 높이기 위함입니다.\n",
    "\n",
    "2) 모델이 보이는 예측 결과에 대한 분석을 용이하게 하기 위함입니다.\n",
    "\n",
    "3) 모델의 다양성을 기반으로 각 모델이 강점을 살려 일반화 성능을 향상 시키기 위함입니다.\n",
    "\n",
    "4) 복잡한 구조의 모델을 단순화 하여 일반화 성능을 높이기 위함입니다. \n",
    "\n",
    "<font color='red'> 정답  3 번: 앙상블을 구성하는 모델의 다변화를 통한 예측 성능의 향상 시키는 방향으로 발전해 왔습니다. </font>\n",
    "\n",
    "\n",
    "## Q2\n",
    "\n",
    "Bagging에 대한 설명으로 틀린 것을 고르세요.\n",
    "\n",
    "1) 불안정한 결과를 모이는 모델의 분산을 줄여 일반화 성능을 높입니다.\n",
    "\n",
    "2) Bootstrap은 Aggregation할 모델들의 다양성을 보이게 합니다.\n",
    "\n",
    "3) Aggregation 모델의 수를 늘릴 때에는 과적합에 유의해야 합니다.\n",
    "\n",
    "4) 단일 모델로 동작할 수 있지만, 일반화 성능이 떨어질 가능성이 높습니다.\n",
    "\n",
    "<font color='red'> 정답 3 번: 평균을 낼 결과의 수가 늘어 나므로 분산이 줄어드는 효과가 있어 과적합 현상을 완화시킬 수 있습니다.</font> \n",
    "\n",
    "\n",
    "## Q3\n",
    "\n",
    "부스팅에 대한 설명으로 적합하지 않은 것은 무엇인가요?\n",
    "\n",
    "1) 경사부스팅의 기반 모델은 분류와 회귀 상관없이 모두 회귀 모델을 사용합니다. \n",
    "\n",
    "2) 부스팅 모델을 구성하는 모델의 수를 늘릴 수록 모델의 편향은 커집니다.\n",
    "\n",
    "3) AdaBoost는 다음 라운드의 모델을 학습할 때 오차가 큰 데이터 포인트의 비중을 높여 주고, 손실을 최소화하는 모델의 가중치를 구합니다.\n",
    "\n",
    "4) 부스팅은 병렬화하여 효율적으로 연산 자원을 활용하기에 용이합니다.\n",
    "\n",
    "<font color='red'> 정답 4: 부스팅은 순차적으로 모델을 결합시키는 순차성을 지니고 있어, 병렬화가 어렵습니다.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becda63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
